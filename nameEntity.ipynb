{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tokenization\n",
    "import run_classifier\n",
    "import optimization\n",
    "import modeling\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class inputExample:\n",
    "    \"\"\"\n",
    "    a input example\n",
    "    \"\"\"\n",
    "    def __init__(self, words, label):\n",
    "        self.token_A = [(w, l) for w, l in zip(words, label)]\n",
    "        \n",
    "    def __str__(self):\n",
    "        index = 0\n",
    "        for ws, ls in self.token_A:\n",
    "            print('word_%s is %s with label %s' % (index, ws, ls))\n",
    "            index += 1\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.token_A)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_tokens(tokens, label_id, max_seq_length):\n",
    "    index = 0\n",
    "    while len(tokens) > max_seq_length:\n",
    "        \n",
    "        if index ==  len(tokens):\n",
    "            i = np.random.randint(len(tokens))\n",
    "            tokens.pop(i)\n",
    "            label_id.pop(i) \n",
    "            index -= 1\n",
    "        else:\n",
    "            for i in range(index,len(tokens)):\n",
    "                index = i+1\n",
    "                if label_id[i] == 'X':\n",
    "                    tokens.pop(i)\n",
    "                    label_id.pop(i)\n",
    "                    index -= 1\n",
    "                    break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_single_example(inputExample, max_seq_length, tokenizer, ex_index, tagtolabel):\n",
    "    \"\"\"\n",
    "    convert existing tokens to Wordpiece tokens\n",
    "    \"\"\"    \n",
    "    def re_tokenize(list_tokens):    \n",
    "           \n",
    "        tokens, labels = [], []\n",
    "        for token_label in list_tokens:\n",
    "            token, label = token_label[0], token_label[1]       \n",
    "            w_token = tokenizer.tokenize(token)\n",
    "            if len(w_token) == 0:\n",
    "                w_label = []\n",
    "            else:\n",
    "                w_label = [label] + ['X'] * (len(w_token) - 1)\n",
    "            \n",
    "            tokens.extend(w_token) \n",
    "            labels.extend(w_label)\n",
    "            \n",
    "        assert len(tokens) == len(labels)\n",
    "        return tokens, labels\n",
    "    \n",
    "    tokens_A, labels_A = re_tokenize(inputExample.token_A)\n",
    "    truncate_tokens(tokens_A, labels_A, max_seq_length -2)\n",
    "        \n",
    "    word_piece_token = ['[CLS]']\n",
    "    labels = ['X']    \n",
    "    \n",
    "    word_piece_token.extend(tokens_A)\n",
    "    labels.extend(labels_A)\n",
    "    \n",
    "    word_piece_token.append('[SEP]')\n",
    "    labels.append('X')\n",
    "    \n",
    "    segment_ids = [0] * len(word_piece_token)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(word_piece_token)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    \n",
    "    # Zero-pad up to the sequence length.   \n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "        labels.append('X')\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "    assert len(labels) == max_seq_length\n",
    "\n",
    "    if ex_index % 3000 == 0:\n",
    "        tf.logging.info(\"*** Example ***\")\n",
    "        tf.logging.info(\"tokens: %s\" % \" \".join([tokenization.printable_text(x) for x in word_piece_token]))\n",
    "        tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "        tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "        tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "        tf.logging.info(\"label: %s\" % labels)\n",
    "    \n",
    "    label_ids = []\n",
    "    for l in labels:\n",
    "        label_ids.append(-1) if l == 'X'else label_ids.append(tagtolabel[l])\n",
    "    \n",
    "    feature = run_classifier.InputFeatures(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids,\n",
    "      label_id=label_ids,\n",
    "      is_real_example=True)\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_based_convert_examples_to_features(examples, max_seq_length, tokenizer, output_file, tagstolabel):\n",
    "\n",
    "    writer = tf.python_io.TFRecordWriter(output_file)\n",
    "    features_out = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 10000 == 0:\n",
    "            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "        feature = convert_single_example(example, max_seq_length, tokenizer, ex_index, tagstolabel)\n",
    "        features_out.append(feature)\n",
    "        def create_int_feature(values):\n",
    "            f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "            return f\n",
    "        \n",
    "        features = collections.OrderedDict()\n",
    "        features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
    "        features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
    "        features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
    "        features[\"label_ids\"] = create_int_feature(feature.label_id)\n",
    "        features[\"is_real_example\"] = create_int_feature([int(feature.is_real_example)])\n",
    "\n",
    "        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "    writer.close()\n",
    "    return features_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_builder(bert_config, num_labels, learning_rate,\n",
    "                     num_train_steps, num_warmup_steps, use_one_hot_embeddings):\n",
    "\n",
    "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "\n",
    "        tf.logging.info(\"*** Features ***\")\n",
    "        for name in sorted(features.keys()):\n",
    "            tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
    "\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids = features[\"label_ids\"]\n",
    "\n",
    "        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "        total_loss, per_example_loss, logits, probabilities = create_model(bert_config, is_training, input_ids, \n",
    "                                                                     input_mask, segment_ids,\n",
    "                                                                label_ids, num_labels, use_one_hot_embeddings)\n",
    "\n",
    "        mask = tf.where(label_ids < 0, tf.zeros(tf.shape(label_ids)), tf.ones(tf.shape(label_ids)))\n",
    "        predictions = tf.cast(tf.argmax(logits, axis=-1), dtype = tf.int32)\n",
    "\n",
    "        batch_accuracy = tf.reduce_sum(tf.cast(tf.equal(predictions, label_ids), dtype = tf.float32) * mask) / tf.reduce_sum(mask)\n",
    "\n",
    "        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "        accuracy = tf.metrics.accuracy(labels=label_ids, predictions=predictions)\n",
    "        loss = tf.metrics.mean(values=per_example_loss)\n",
    "        \n",
    "        eval_metrics = {\"eval_accuracy\": accuracy, \"eval_loss\": loss}\n",
    "\n",
    "        output_spec = None\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            \n",
    "            train_op = optimization.create_optimizer(\n",
    "                  total_loss, learning_rate, num_train_steps, num_warmup_steps, False)\n",
    "            \n",
    "            output_spec = tf.estimator.EstimatorSpec(mode, loss = total_loss, train_op = train_op, eval_metric_ops = eval_metrics)\n",
    "            \n",
    "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "            \n",
    "            output_spec = tf.estimator.EstimatorSpec(mode, loss = total_loss, \n",
    "                                                     eval_metric_ops = eval_metrics)\n",
    "        else:\n",
    "            output_spec = tf.estimator.EstimatorSpec(mode,  \n",
    "                                                     predictions={\"probabilities\": probabilities})\n",
    "            \n",
    "        return output_spec\n",
    "\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_based_input_fn_builder(input_file, seq_length, is_training,\n",
    "                                drop_remainder, num_data_to_use = 128, batch_size = 32):\n",
    "\n",
    "    name_to_features = {\n",
    "      \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "      \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "      \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "      \"label_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "      \"is_real_example\": tf.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "\n",
    "    def _decode_record(record, name_to_features):\n",
    "        example = tf.parse_single_example(record, name_to_features)\n",
    "\n",
    "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "    # So cast all int64 to int32.\n",
    "        for name in list(example.keys()):\n",
    "            t = example[name]\n",
    "            if t.dtype == tf.int64:\n",
    "                t = tf.to_int32(t)\n",
    "            example[name] = t\n",
    "\n",
    "        return example    \n",
    "    # For training, we want a lot of parallel reading and shuffling.\n",
    "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "    d = tf.data.TFRecordDataset(input_file)\n",
    "    d = d.take(num_data_to_use).cache()\n",
    "    if is_training:\n",
    "        d = d.shuffle(buffer_size=100)\n",
    "\n",
    "    d = d.apply(\n",
    "        tf.contrib.data.map_and_batch(\n",
    "            lambda record: _decode_record(record, name_to_features),\n",
    "            batch_size=batch_size,\n",
    "            drop_remainder=drop_remainder))        \n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocan_file = 'bert_model/vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenization.FullTokenizer(\n",
    "vocab_file=vocan_file, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/ner.csv.zip'\n",
    "dataset_path = 'data/ner_dataset.csv.zip'\n",
    "data = pd.read_csv(dataset_path, encoding = 'latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = data['Tag'].unique()\n",
    "tagstolabel = {t:i for t, i in zip(tags, range(len(tags)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covert_data_to_Example(data):\n",
    "    list_tokens = []\n",
    "    tokens, lables = [data.loc[0,'Word']], [data.loc[0,'Tag']]\n",
    "    pre_sentence_no = data.loc[0,'Sentence #'].split(' ')\n",
    "\n",
    "    for index, row in data[1:].iterrows():\n",
    "        sentence_index = row['Sentence #']\n",
    "        word = row['Word']\n",
    "        tag = row['Tag']\n",
    "\n",
    "        if isinstance(sentence_index, str):\n",
    "            sentence_no = sentence_index.split(' ')[1] \n",
    "            inputexmplae = inputExample(tokens, lables)\n",
    "            list_tokens.append(inputexmplae)\n",
    "            tokens, lables = [word], [tag]\n",
    "\n",
    "        else:\n",
    "            tokens.append(word)\n",
    "            lables.append(tag)\n",
    "    return list_tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tokens = covert_data_to_Example(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.864277075774638\n",
      "104\n"
     ]
    }
   ],
   "source": [
    "len_tokens = [len(example.token_A) for example in list_tokens]\n",
    "print(np.mean(len_tokens))\n",
    "print(np.max(len_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 47958\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] thousands of demonstrators have marched through london to protest the war in iraq and demand the withdrawal of british troops from that country . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 5190 1997 28337 2031 9847 2083 2414 2000 6186 1996 2162 1999 5712 1998 5157 1996 10534 1997 2329 3629 2013 2008 2406 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: ['X', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X']\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] the du ##ja ##il case is widely seen as relatively un ##com ##pl ##icated compared to cases of alleged genocide and crimes against humanity still under investigation . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1996 4241 3900 4014 2553 2003 4235 2464 2004 4659 4895 9006 24759 17872 4102 2000 3572 1997 6884 14052 1998 6997 2114 8438 2145 2104 4812 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: ['X', 'O', 'B-geo', 'X', 'X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'X', 'X', 'X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X']\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] she is at least the seventh person to die this year from the disease . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2016 2003 2012 2560 1996 5066 2711 2000 3280 2023 2095 2013 1996 4295 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: ['X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-tim', 'O', 'B-tim', 'O', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X']\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] in 2005 , zambia qualified for debt relief under the highly ind ##eb ##ted poor country initiative , consisting of approximately usd 6 billion in debt relief . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1999 2384 1010 15633 4591 2005 7016 4335 2104 1996 3811 27427 15878 3064 3532 2406 6349 1010 5398 1997 3155 13751 1020 4551 1999 7016 4335 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: ['X', 'O', 'B-tim', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'B-org', 'I-org', 'X', 'X', 'I-org', 'I-org', 'I-org', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X']\n",
      "INFO:tensorflow:Writing example 10000 of 47958\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] when their strife was at its height , a bram from the neighboring hedge lifted up its voice , and said in a bo tone : \" pray , my dear friends , in my presence at least cease from such vain di ##sp ##uting ##s . \" [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2043 2037 27865 2001 2012 2049 4578 1010 1037 20839 2013 1996 8581 17834 4196 2039 2049 2376 1010 1998 2056 1999 1037 8945 4309 1024 1000 11839 1010 2026 6203 2814 1010 1999 2026 3739 2012 2560 13236 2013 2107 15784 4487 13102 20807 2015 1012 1000 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: ['X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'X', 'X', 'X', 'O', 'O', 'X']\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] a number of reins ##urance companies relocated to the island following the 11 september 2001 attacks and again after hurricane katrina in august 2005 contributing to the expansion of an already robust international business sector . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1037 2193 1997 19222 25863 3316 7448 2000 1996 2479 2206 1996 2340 2244 2541 4491 1998 2153 2044 7064 16864 1999 2257 2384 8020 2000 1996 4935 1997 2019 2525 15873 2248 2449 4753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: ['X', 'O', 'O', 'O', 'O', 'X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-tim', 'I-tim', 'I-tim', 'O', 'O', 'O', 'O', 'O', 'B-nat', 'O', 'B-tim', 'I-tim', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X']\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] china participated for the fist time , cutting the lights at beijing ' s bird ' s nest stadium and the water cube . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2859 4194 2005 1996 7345 2051 1010 6276 1996 4597 2012 7211 1005 1055 4743 1005 1055 9089 3346 1998 1996 2300 14291 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: ['X', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'X', 'B-org', 'I-org', 'X', 'I-org', 'I-org', 'O', 'O', 'B-org', 'I-org', 'O', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X']\n",
      "INFO:tensorflow:Writing example 20000 of 47958\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] the xi ##nh ##ua news agency reports the resolution noted that taiwan ' s leaders have accelerated what it termed \" dangerous \" steps toward independence . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1996 8418 25311 6692 2739 4034 4311 1996 5813 3264 2008 6629 1005 1055 4177 2031 14613 2054 2009 12061 1000 4795 1000 4084 2646 4336 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: ['X', 'O', 'B-org', 'X', 'X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X']\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] mr . ol ##mer ##t said he believes such a move would be in israel ' s interest . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2720 1012 19330 5017 2102 2056 2002 7164 2107 1037 2693 2052 2022 1999 3956 1005 1055 3037 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: ['X', 'B-per', 'X', 'I-per', 'X', 'X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'X', 'O', 'O', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X']\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] later in the day , nine construction workers were killed in a separate roadside bombing in the bar kun ##ar district of kun ##ar province . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2101 1999 1996 2154 1010 3157 2810 3667 2020 2730 1999 1037 3584 25131 8647 1999 1996 3347 28919 2906 2212 1997 28919 2906 2874 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: ['X', 'B-tim', 'I-tim', 'O', 'B-tim', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'I-geo', 'X', 'O', 'O', 'B-geo', 'X', 'O', 'O', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X']\n",
      "INFO:tensorflow:Writing example 30000 of 47958\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] hamas wants to build a coalition government , but mr . abbas ' s fat ##ah party and the militant islamic jihad group have declined to join . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 22129 4122 2000 3857 1037 6056 2231 1010 2021 2720 1012 17532 1005 1055 6638 4430 2283 1998 1996 16830 5499 24815 2177 2031 6430 2000 3693 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: ['X', 'B-org', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-per', 'X', 'I-per', 'O', 'X', 'B-geo', 'X', 'O', 'O', 'O', 'O', 'B-geo', 'I-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X']\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] this being done he was able to di ##sse ##mble his resentment with a sign of affection , and the earth was his and the fu ##ln ##ess thereof . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2023 2108 2589 2002 2001 2583 2000 4487 11393 19661 2010 20234 2007 1037 3696 1997 12242 1010 1998 1996 3011 2001 2010 1998 1996 11865 19666 7971 21739 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: ['X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'X', 'X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'X', 'X', 'O', 'O', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X']\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] iran ' s supreme leader says he sees no benefit to negotiations with the united states on his country ' s nuclear program . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 4238 1005 1055 4259 3003 2758 2002 5927 2053 5770 2000 7776 2007 1996 2142 2163 2006 2010 2406 1005 1055 4517 2565 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: ['X', 'B-geo', 'O', 'X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'I-geo', 'O', 'O', 'O', 'O', 'X', 'O', 'O', 'O', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X']\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] a u . s . navy plane was destroyed tuesday when it overs ##hot the runway at bag ##ram air base in afghanistan . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1037 1057 1012 1055 1012 3212 4946 2001 3908 9857 2043 2009 15849 12326 1996 9271 2012 4524 6444 2250 2918 1999 7041 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: ['X', 'O', 'B-org', 'X', 'X', 'X', 'I-org', 'O', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'X', 'O', 'O', 'O', 'B-org', 'X', 'I-org', 'I-org', 'O', 'B-geo', 'O', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X']\n",
      "INFO:tensorflow:Writing example 40000 of 47958\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] travel for this year ' s thanksgiving holiday in the united states is expected to reach levels not seen since the september 11 terrorist attacks three years ago . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 3604 2005 2023 2095 1005 1055 15060 6209 1999 1996 2142 2163 2003 3517 2000 3362 3798 2025 2464 2144 1996 2244 2340 9452 4491 2093 2086 3283 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: ['X', 'O', 'O', 'O', 'O', 'B-tim', 'X', 'I-tim', 'I-tim', 'I-tim', 'O', 'B-geo', 'I-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-tim', 'I-tim', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X']\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] when mr . dos ##tum ' s appointment was announced last month , it drew considerable criticism . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2043 2720 1012 9998 11667 1005 1055 6098 2001 2623 2197 3204 1010 2009 3881 6196 6256 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: ['X', 'O', 'B-per', 'X', 'I-per', 'X', 'O', 'X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X']\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 50\n",
    "output_file = 'data_file'\n",
    "# features = convert_examples_to_features(list_tokens, max_seq_length, tokenizer, tagstolabel)\n",
    "features = file_based_convert_examples_to_features(list_tokens, max_seq_length, tokenizer, output_file, tagstolabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47958"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del list_tokens\n",
    "del features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
    "                 labels, num_labels, use_one_hot_embeddings):\n",
    "\n",
    "    model = modeling.BertModel(\n",
    "      config=bert_config,\n",
    "      is_training=is_training,\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      token_type_ids=segment_ids,\n",
    "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "#     output_layer = model.get_pooled_output()\n",
    "    output_layer = model.get_sequence_output()\n",
    "    timesteps = output_layer.shape[1].value\n",
    "\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        if is_training:\n",
    "          # I.e., 0.1 dropout\n",
    "            output_layer = tf.nn.dropout(output_layer, keep_prob=0.8)\n",
    "            \n",
    "        logits = tf.layers.dense(output_layer, units = num_labels)\n",
    "        probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        \n",
    "#  label = -1 will be converted to zeros [0,0,0,0...]\n",
    "        one_hot_labels = tf.one_hot(labels, depth = num_labels)\n",
    "           \n",
    "        per_example_loss = tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "#         do not make prediction on 'X' mark\n",
    "        per_example_loss = tf.where(labels < 0, tf.stop_gradient(per_example_loss), per_example_loss)\n",
    "        per_example_loss = tf.reduce_sum(per_example_loss, axis = -1)\n",
    "        loss = - tf.reduce_mean(per_example_loss)\n",
    "\n",
    "    return (loss, per_example_loss, logits, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feed_dict(features):\n",
    "\n",
    "    f_input_ids = []\n",
    "    f_input_mask = []\n",
    "    f_segment_ids = []\n",
    "    f_label_ids = []\n",
    "    \n",
    "    for feature in features:\n",
    "        f_input_ids.append(feature.input_ids)\n",
    "        f_input_mask.append(feature.input_mask)\n",
    "        f_segment_ids.append(feature.segment_ids)\n",
    "        f_label_ids.append(feature.label_id)\n",
    "    \n",
    "    feed_dict = {'input_id:0': f_input_ids,\n",
    "                'input_mask:0': f_input_mask,\n",
    "                'segment_ids:0': f_segment_ids,\n",
    "                'label_ids:0': f_label_ids }\n",
    "    \n",
    "    return feed_dict\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataSet(features, is_training, batch_size = 32):\n",
    "    \n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    label_ids = features[\"label_ids\"]\n",
    "\n",
    "    d = tf.data.Dataset.from_tensor_slices((input_ids,input_mask,segment_ids,label_ids))\n",
    "\n",
    "    if is_training:\n",
    "        d = d.shuffle(buffer_size=100)\n",
    "\n",
    "    d = d.batch(batch_size=batch_size)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config_file = 'bert_model/bert_config.json'\n",
    "bert_config = modeling.BertConfig.from_json_file(bert_config_file)\n",
    "is_training = True\n",
    "use_one_hot_embeddings = True\n",
    "epoch = 30\n",
    "batch_size = 32\n",
    "max_seq_length = 50\n",
    "input_file = 'data_file'\n",
    "num_labels = len(tags)\n",
    "learning_rate = 0.03\n",
    "num_train_steps = 30\n",
    "num_warmup_steps = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# feature_dict = { 'input_ids':tf.placeholder(dtype = tf.int32, shape = [None, max_seq_length], name = 'input_id'),\n",
    "#                 'input_mask':tf.placeholder(dtype = tf.int32, shape = [None, max_seq_length], name = 'input_mask'),\n",
    "#                 'segment_ids': tf.placeholder(dtype = tf.int32, shape = [None, max_seq_length], name = 'segment_ids'),\n",
    "#                'label_ids': tf.placeholder(dtype = tf.int32, shape = [None, max_seq_length], name = 'label_ids' )}\n",
    "# train_data = buildDataSet(feature_dict, is_training, batch_size)\n",
    "\n",
    "train_data = file_based_input_fn_builder(input_file, max_seq_length, is_training, drop_remainder = True, num_data_to_use = 1000)\n",
    "\n",
    "iterator = train_data.make_initializable_iterator()\n",
    "features = iterator.get_next()\n",
    "\n",
    "input_ids = features['input_ids']\n",
    "input_mask = features['input_mask']\n",
    "segment_ids = features['segment_ids']\n",
    "label_ids = features['label_ids']\n",
    "\n",
    "total_loss, per_example_loss, logits, probabilities = create_model(bert_config, is_training, input_ids, \n",
    "                                                             input_mask, segment_ids,\n",
    "                                                        label_ids, num_labels, use_one_hot_embeddings)\n",
    "\n",
    "mask = tf.where(label_ids < 0, tf.zeros(tf.shape(label_ids)), tf.ones(tf.shape(label_ids)))\n",
    "predictions = tf.cast(tf.argmax(logits, axis=-1), dtype = tf.int32)\n",
    "\n",
    "batch_accuracy = tf.reduce_sum(tf.cast(tf.equal(predictions, label_ids), dtype = tf.float32) * mask) / tf.reduce_sum(mask)\n",
    "acc, acc_op = tf.metrics.accuracy(labels=label_ids, predictions=predictions, weights = mask)\n",
    "# loss, loss_op = tf.metrics.mean(values=per_example_loss)\n",
    "# train_op = optimization.create_optimizer(total_loss, learning_rate, num_train_steps, num_warmup_steps, False)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 0, loss is 92.76889, accuracy is 0.6759771\n",
      "at epoch 1, loss is 23.623928, accuracy is 0.8380643\n",
      "at epoch 2, loss is 15.098858, accuracy is 0.8534762\n",
      "at epoch 3, loss is 15.308939, accuracy is 0.8531937\n",
      "at epoch 4, loss is 17.861944, accuracy is 0.85430056\n",
      "at epoch 5, loss is 17.915977, accuracy is 0.8530524\n",
      "at epoch 6, loss is 17.395687, accuracy is 0.8534686\n",
      "at epoch 7, loss is 17.265427, accuracy is 0.8537034\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-12f385e663d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_accu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = num_train_steps\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(epoch):\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        sess.run(iterator.initializer)\n",
    "        try: \n",
    "            while True:\n",
    "                _, los, accuracy, batch_accu, _ = sess.run([train_op, total_loss, acc, batch_accuracy, acc_op])\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        print('at epoch %s, loss is %s, accuracy is %s' % (i, los, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
